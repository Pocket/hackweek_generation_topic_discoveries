{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Kaggle token and place a kaggle.json file and run this to connect\n",
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "! kaggle datasets list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data from kaggle\n",
    "! kaggle datasets list -s criteo\n",
    "# ! kaggle datasets download -d mrkmakr/criteo-dataset \n",
    "! kaggle datasets download -d benediktschifferer/criteo-dataset-parquet\n",
    "! mv criteo-dataset.zip data/\n",
    "! mv criteo-dataset-parquet.zip data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the test.txt.zip to the gcs (one time)\n",
    "# I had to download the test.txt.zip from the \n",
    "# Note a big training set is here: https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/\n",
    "# https://www.kaggle.com/code/rikdifos/criteo-ctr-baseline/input?select=dac\n",
    "from google.cloud import storage\n",
    "\n",
    "def upload_file_to_gcs(FILENAME):\n",
    "    LOCAL_PATH =\"./data\"\n",
    "\n",
    "    PROJECT_ID = !(gcloud config get-value core/project)\n",
    "    PROJECT_ID = PROJECT_ID[0]\n",
    "    REGION = 'us-west1'\n",
    "    GCS_BUCKET = f\"{PROJECT_ID}-bucket\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(GCS_BUCKET)\n",
    "    blob = bucket.blob(f\"criteo/{FILENAME}\")\n",
    "    logging.info('Uploading local csv file to GCS...')\n",
    "    blob.upload_from_filename(f\"{LOCAL_PATH}/{FILENAME}\")\n",
    "\n",
    "# upload_file_to_gcs(\"test.txt.zip\")\n",
    "# upload_file_to_gcs(\"dac_sample.tar.gz\")\n",
    "upload_file_to_gcs(\"criteo-dataset.zip\")\n",
    "upload_file_to_gcs(\"criteo-dataset-parquet.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepctr_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr_torch.models import *\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import warnings as w\n",
    "w.filterwarnings(action='ignore')\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper link: https://arxiv.org/pdf/1703.04247.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T05:49:32.897744Z",
     "iopub.status.busy": "2023-07-17T05:49:32.896763Z",
     "iopub.status.idle": "2023-07-17T05:49:34.498813Z",
     "shell.execute_reply": "2023-07-17T05:49:34.497791Z",
     "shell.execute_reply.started": "2023-07-17T05:49:32.897703Z"
    }
   },
   "source": [
    "!wget -P data -c https://labs.criteo.com/wp-content/uploads/2015/04/dac_sample.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Explain\n",
    "   1. sparse_feature : I1 ~ I27\n",
    "   2. dense_feature : C1 ~ C14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['label', *(f'I{i}' for i in range(1, 14)), *(f'C{i}' for i in range(1, 27))]\n",
    "data = pd.read_csv('data/dac_sample.txt', sep='\\t', names=columns).fillna(0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts().plot(kind='bar',figsize=(10,8))\n",
    "print(data.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical data apply Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "labeling = LabelEncoder()\n",
    "data.iloc[::,1:14] = scaler.fit_transform(data.iloc[::,1:14])\n",
    "dense_feature = data.iloc[::,14:40] \n",
    "for feature in dense_feature:\n",
    "    fix_data = []\n",
    "    for variable in dense_feature[feature]:\n",
    "        if variable == 0:\n",
    "            variable = 'a'\n",
    "            fix_data.append(variable)\n",
    "        else:\n",
    "            fix_data.append(variable)\n",
    "    dense_feature[feature] = fix_data\n",
    "    dense_feature[feature] = labeling.fit_transform(dense_feature[feature])\n",
    "    data.iloc[::,1:14] = scaler.fit_transform(data.iloc[::,1:14])\n",
    "data.iloc[::,14:40] = dense_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "    dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "    \n",
    "    target = ['label']\n",
    "\n",
    "    \n",
    "    fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                              for feat in sparse_features] + [DenseFeat(feat, 1, )\n",
    "                                                              for feat in dense_features]\n",
    "\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    feature_names = get_feature_names(\n",
    "        linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    # 3.generate input data for model\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=2020)\n",
    "    train_model_input = {name: train[name] for name in feature_names}\n",
    "    test_model_input = {name: test[name] for name in feature_names}\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "\n",
    "    device = 'cpu'\n",
    "    use_cuda = True\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        print('cuda ready...')\n",
    "        device = 'cuda:0'\n",
    "\n",
    "    model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns,\n",
    "                   task='binary',\n",
    "                   l2_reg_embedding=1e-5, device=device)\n",
    "\n",
    "    model.compile(\"adagrad\", \"binary_crossentropy\",\n",
    "                  metrics=[\"binary_crossentropy\", \"auc\"], )\n",
    "\n",
    "    history = model.fit(train_model_input, train[target].values, batch_size=512, epochs=50, verbose=1,\n",
    "                        validation_split=0.2)\n",
    "    pred_ans = model.predict(test_model_input, 256)\n",
    "    print(\"\")\n",
    "    print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "    print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pd.DataFrame.from_dict(history.history)\n",
    "display(loss.head())\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(loss.binary_crossentropy,label='Train Loss')\n",
    "plt.plot(loss.val_binary_crossentropy,label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(loss.auc,label='Train AUC')\n",
    "plt.plot(loss.val_auc,label='Validation AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why this dataset happened overfitting?\n",
    "### But in movie len dataset this DeepFM good performance\n",
    "### Movie Len : https://www.kaggle.com/code/leejunseok97/deepfm-movie-len-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FM(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         ix = (torch.sum(x,dim=1) ** 2) - (torch.sum(x ** 2,dim=1))\n",
    "#         ix = torch.sum(ix,dim=1,keepdim=True)\n",
    "#         return 0.5 *  ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class feat_linear(nn.Module):\n",
    "#     def __init__(self,field_dim_list):\n",
    "#         super().__init__()\n",
    "#         self.fc = nn.Embedding(sum(field_dim_list),1)\n",
    "#         self.bias = nn.Parameter(torch.zeros((1,)))\n",
    "#         self.offsets = np.array((0, *np.cumsum(field_dim_list)[:-1]),dtype=np.long)\n",
    "#     def forward(self,x):\n",
    "#         x += x.new_tensor(self.offsets).unsqueeze(0)\n",
    "#         return torch.sum(self.fc(x),dim=1) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class feat_embedding(nn.Module):\n",
    "#     def __init__(self,field_dim_list,emb_dim):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(sum(field_dim_list),emb_dim)\n",
    "#         self.offsets = np.array((0, *np.cumsum(field_dim_list)[:-1]),dtype=np.long)\n",
    "#         nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "#     def forward(self,x):\n",
    "#         x += x.new_tensor(self.offsets).unsqueeze(0)\n",
    "#         return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     def __init__(self,input_dim,embed_dim):\n",
    "#         super().__init__()\n",
    "#         self.seq = nn.Sequential(\n",
    "#             nn.Linear(input_dim,embed_dim),\n",
    "#             nn.BatchNorm1d(embed_dim),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(embed_dim,1)\n",
    "#         )\n",
    "#     def forward(self,x):\n",
    "#         return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DeepFM(nn.Module):\n",
    "#     def __init__(self,field_dim_list,embed_dim,mlp_dims):\n",
    "#         super().__init__()\n",
    "#         self.linear = feat_linear(field_dim_list)\n",
    "#         self.fm = FM()\n",
    "#         self.embedding = feat_embedding(field_dim_list,emb_dim=embed_dim)\n",
    "#         self.embed_output_dim = len(field_dim_list) * embed_dim\n",
    "#         self.mlp = MLP(self.embed_output_dim , mlp_dims)\n",
    "#     def forward(self,x):\n",
    "#         embed_x = self.embedding(x)\n",
    "#         output = self.linear(x) + self.fm(embed_x) + self.mlp(embed_x.view(-1,self.embed_output_dim))\n",
    "#         return torch.sigmoid(output.squeeze(1))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
