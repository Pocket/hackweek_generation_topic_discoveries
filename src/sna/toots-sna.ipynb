{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d55da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import os\n",
    "import os, json, openai, warnings, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "import networkx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout \n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dfcf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Called  https://mastodon.social/api/v2/instance\n",
    "instance_stats = {\n",
    "  \"domain\": \"mastodon.social\",\n",
    "  \"title\": \"Mastodon\",\n",
    "  \"version\": \"4.1.2+nightly-20230627\",\n",
    "  \"source_url\": \"https://github.com/mastodon/mastodon\",\n",
    "  \"description\": \"The original server operated by the Mastodon gGmbH non-profit\",\n",
    "  \"usage\": {\n",
    "    \"users\": {\n",
    "      \"active_month\": 221664\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e3221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# s3://ml-team-pocket/social_hackweek_2023/topic_discoveries/\n",
    "# I used replied_toots_2023_05_27.tar.gz and untar into data folder.\n",
    "data_path = \"../../data\"\n",
    "base_path = \"{}/replied_toots_2023_05_27\".format(data_path)\n",
    "datasets = glob.glob(\"{}/toots_mastodon*.parquet\".format(base_path))\n",
    "toots_df = pd.concat([pd.read_parquet(data) for data in datasets], axis=0).reset_index(drop=True)\n",
    "print(len(toots_df))\n",
    "toots_df = toots_df.drop_duplicates(subset=['id'])\n",
    "display(toots_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac030e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = glob.glob(\"{}/status_mastodon*.parquet\".format(base_path))\n",
    "statuses_df = pd.concat([pd.read_parquet(data) for data in datasets], axis=0).reset_index(drop=True)\n",
    "statuses_df = statuses_df[(statuses_df['content'].apply(len) < 256) & (statuses_df['language'] == 'en') & (statuses_df['content'] != '' )]\n",
    "statuses_df = statuses_df.drop_duplicates(subset=['id'])\n",
    "len(statuses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f102adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accts = toots_df['account'].apply(lambda acc: acc['acct'])\n",
    "display(toots_df['replies_count'].value_counts(), toots_df['reblogs_count'].value_counts(), toots_df['favourites_count'].value_counts())\n",
    "\n",
    "replies_only_df = toots_df.loc[toots_df['replies_count'] > 0]\n",
    "# out of 167K, only 20K have replies, and 99% are less then 1.\n",
    "# sns.histplot(data=replies_only_df, y=\"replies_count\")\n",
    "\n",
    "display(toots_df.describe())\n",
    "display(replies_only_df.describe())\n",
    "replies_only_df_pruned = toots_df.loc[toots_df['replies_count'] > 5]\n",
    "sns.histplot(data=replies_only_df_pruned, x=\"replies_count\", binwidth=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88724c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "\n",
    "def create_graph_from_status(_statuses_df, min_degree=0, max_degree=2**32):\n",
    "    # Lets collate things by the influencer nodes \n",
    "    influencers = defaultdict(set)\n",
    "    \n",
    "    for k, status in _statuses_df.iterrows():\n",
    "        if status['parent_account_id'] == None: # just a very few this is not filled incorrectly i believe\n",
    "            continue\n",
    "        if status['parent_account_id'] == status['account']['id']: #self\n",
    "            continue\n",
    "        influencers[status['parent_account_id']].add(status['account']['id'])\n",
    "    \n",
    "    # pruning\n",
    "    to_delete = []\n",
    "    for influencer, followers in influencers.items():\n",
    "        if min_degree and len(followers) < min_degree or len(followers) > max_degree:\n",
    "            to_delete.append(influencer)\n",
    "        \n",
    "    for infl in to_delete:\n",
    "        del influencers[infl]\n",
    "    \n",
    "    G=nx.Graph()\n",
    "    for dest, edges in influencers.items():\n",
    "        for src in edges:\n",
    "            G.add_edge(src, dest)\n",
    "            \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdbce0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution of degree\n",
    "status_gph = create_graph_from_status(statuses_df)\n",
    "deg=nx.degree(status_gph)\n",
    "\n",
    "degree_sequence = sorted((d for n, d in status_gph.degree()), reverse=True)\n",
    "dmax = max(degree_sequence)\n",
    "\n",
    "fig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n",
    "# Create a gridspec for adding subplots of different sizes\n",
    "axgrid = fig.add_gridspec(5, 4)\n",
    "\n",
    "ax2 = fig.add_subplot(axgrid[3:, 2:])\n",
    "ax2.bar(*np.unique(degree_sequence, return_counts=True))\n",
    "ax2.set_title(\"Degree histogram\")\n",
    "ax2.set_xlabel(\"Degree\")\n",
    "ax2.set_ylabel(\"# of Nodes\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ea74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Stats\n",
    "import math\n",
    "\n",
    "total_toots = len(toots_df)\n",
    "replies_count_df = toots_df.loc[toots_df['replies_count'] > 0]\n",
    "with_replies_pct = int(len(replies_count_df)/total_toots * 100)\n",
    "median = replies_count_df['replies_count'].median()\n",
    "\n",
    "print(\"\"\"\n",
    "Total Toots: {}\n",
    "With Replies: {}%\n",
    "Median Replies: {}\"\"\".format(total_toots, with_replies_pct, median))\n",
    "\n",
    "median_degree = degree_sequence[int(len(degree_sequence) / 2)]\n",
    "avg_degree = sum(degree_sequence)/len(degree_sequence)\n",
    "\n",
    "total_threads = (statuses_df['parent_account_id'] != statuses_df['parent_account_id'].shift(axis=0)).sum(axis=0)\n",
    "# closeness = nx.closeness_centrality(G) # This is slow\n",
    "# Average Closness {} hops - need to convert to hops - otherwise doesn't make sense\n",
    "cluster_coefficient = nx.average_clustering(status_gph)\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "Total Threads: {} \n",
    "Median Degree: {} connn/pp\n",
    "Avg Degree: {}\n",
    "Cluster Coefficient {}%\"\"\".format(total_threads, median_degree,  avg_degree, round(cluster_coefficient * 100, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e012a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_kamada(G):\n",
    "    nodes = G.nodes()\n",
    "    degree = G.degree()\n",
    "    colors = [degree[n] for n in nodes]\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "    #pos = nx.spring_layout(G, k = 0.2)\n",
    "    cmap = plt.cm.viridis_r\n",
    "    cmap = plt.cm.Greys\n",
    "\n",
    "    vmin = min(colors)\n",
    "    vmax = max(colors)\n",
    "\n",
    "    fig = plt.figure(figsize = (15,9), dpi=100)\n",
    "\n",
    "    nx.draw(G,pos,alpha = 0.8, nodelist = nodes, node_color = 'w', node_size = 10, with_labels= False,font_size = 6, width = 0.2, cmap = cmap, edge_color ='yellow')\n",
    "    fig.set_facecolor('#0B243B')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_centrality_1(G, fast=False):\n",
    "\n",
    "    degrees_df = pd.DataFrame.from_dict({node: val for node, val in nx.degree(status_gph)}, orient='index')\n",
    "    degrees_df.index.names=['ID']\n",
    "    degrees_df.columns = ['degree']\n",
    "    degrees_df.reset_index(level=0, inplace=True)\n",
    "    analyse = degrees_df\n",
    "\n",
    "    # Betweenness centrality \n",
    "    if not fast:\n",
    "        bet_cen = nx.betweenness_centrality(G)\n",
    "        df_bet_cen = pd.DataFrame.from_dict(bet_cen, orient='index')\n",
    "        df_bet_cen.columns = ['betweenness_centrality']\n",
    "        df_bet_cen.index.names = ['ID']\n",
    "        df_bet_cen.reset_index(level=0, inplace=True)\n",
    "        analyse= pd.merge(analyse, df_bet_cen, on = ['ID'])\n",
    "\n",
    "    # Clustering coefficient \n",
    "    clust_coefficients = nx.clustering(G)\n",
    "    df_clust = pd.DataFrame.from_dict(clust_coefficients, orient='index')\n",
    "    df_clust.columns = ['clust_coefficient']\n",
    "    df_clust.index.names = ['ID']\n",
    "    df_clust.reset_index(level=0, inplace=True)\n",
    "    analyse= pd.merge(analyse, df_clust, on = ['ID'])\n",
    "\n",
    "    # Closeness centrality \n",
    "    if not fast:\n",
    "        clo_cen = nx.closeness_centrality(G)\n",
    "        df_clo = pd.DataFrame.from_dict(clo_cen, orient='index')\n",
    "        df_clo.columns = ['closeness_centrality']\n",
    "        df_clo.index.names = ['ID']\n",
    "        df_clo.reset_index(level=0, inplace=True)\n",
    "        analyse= pd.merge(analyse, df_clo, on = ['ID'])\n",
    "\n",
    "    # Eigenvector centrality\n",
    "    #eig_cen = nx.eigenvector_centrality(G)\n",
    "    #eig_cen = nx.katz_centrality(G)\n",
    "    eig_cen = nx.eigenvector_centrality_numpy(G)\n",
    "    df_eig = pd.DataFrame.from_dict(eig_cen, orient='index')\n",
    "    df_eig.columns = ['eigenvector_centrality']\n",
    "    df_eig.index.names = ['ID']\n",
    "    df_eig.reset_index(level=0, inplace=True)\n",
    "    analyse= pd.merge(analyse, df_eig, on = ['ID'])\n",
    "    return analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort betweeness and degree.  A node can have a lot of connection, but influence various by WHO you are connected to.\n",
    "# Downloaded centrality_15K.parquet from se\n",
    "should_regenerate = False\n",
    "if should_regenerate:\n",
    "    status_gph = create_graph_from_status(statuses_df, False)\n",
    "    centrality = analyze_centrality_1(status_gph)\n",
    "    centrality.to_parquet(\"{}/results/centrality_15K.parquet\".format(data_path))\n",
    "else:\n",
    "    centrality = pd.read_parquet(\"{}/results/centrality_15K.parquet\".format(data_path))\n",
    "\n",
    "print('Total Results {}'.format(len(status_gph)))\n",
    "median_degree = centrality['degree']\n",
    "print('Top 25 Sorted by Betweeness')\n",
    "display(centrality.sort_values('betweenness_centrality', ascending=False).head(25))\n",
    "\n",
    "print('Top 25 Sorted by  Degree')\n",
    "display(centrality.sort_values('degree', ascending=False).head(50))\n",
    "\n",
    "\n",
    "# Note to lookup ID - we need to use the `/api/v1/accounts` and look up the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d8e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_from_user(user_id, degrees_to_search = 2):\n",
    "    _connector_df = statuses_df\n",
    "    _connector_nodes = _connector_df.loc[_connector_df['parent_account_id'] == user_id].copy()\n",
    "    _connector_nodes['degree'] = 1\n",
    "    for deg in range(0, degrees_to_search):\n",
    "        fofs_ids = _connector_nodes['account'].apply(lambda acc: acc['id']).unique()\n",
    "        fofs_nodes = _connector_df.loc[_connector_df['parent_account_id'].isin(fofs_ids)].copy()\n",
    "        fofs_nodes['degree'] = deg + 2\n",
    "        _connector_nodes = pd.concat([_connector_nodes, fofs_nodes], axis=0)\n",
    "        \n",
    "    _connector_gph = create_graph_from_status(_connector_nodes, False)\n",
    "    return _connector_nodes, _connector_gph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60cf35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Can I just look at how this is connecting the 2?\n",
    "\n",
    "# Take a look at 3 degrees\n",
    "high_betweeness = '109447331150259202'\n",
    "connector_nodes, connector_gph = get_graph_from_user(high_betweeness, 3)\n",
    "draw_kamada(connector_gph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230816b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can I just look at how this is connecting the 2?\n",
    "\n",
    "# Take a look at 3 degrees\n",
    "# less_connector_nodes = statuses_df.loc[statuses_df['parent_account_id'] == \"30437\"]\n",
    "\n",
    "low_betweeness = '30437'\n",
    "# low_betweeness = '38659'\n",
    "less_connector_nodes, less_connector_gph = get_graph_from_user(low_betweeness, 3)\n",
    "draw_kamada(less_connector_gph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD we did 35K reply threads\n",
    "topics_threads = pd.read_parquet('{}/results/df_for_discovery_35k_full.parquet'.format(data_path))\n",
    "print(len(topics_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_threads['topics_single'] = topics_threads['topics'].apply(lambda t: t[0] if len(t) > 0  else '')\n",
    "merged_topics = connector_nodes.merge(topics_threads, left_on='id', right_on='id', how = 'inner')\n",
    "display(merged_topics.describe())\n",
    "display(merged_topics.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: notes is from account.  Content is the main content and is stemmed/stopword removed\n",
    "# Here's we can join centrality matrix and give the top eigenvector \n",
    "\n",
    "\n",
    "def get_topics_by_degree(_connector_nodes):\n",
    "    merged_topics = _connector_nodes.merge(topics_threads, left_on='id', right_on='id', how = 'inner')\n",
    "    merged_topics = merged_topics.loc[merged_topics['topics_single'] != \"\"]\n",
    "\n",
    "    first_degree = merged_topics.loc[merged_topics['degree'] == 1]\n",
    "    second_degree = merged_topics.loc[merged_topics['degree'] == 2]\n",
    "    third_degree = merged_topics.loc[merged_topics['degree'] == 3]\n",
    "\n",
    "\n",
    "    display(first_degree['topics_single'].value_counts())\n",
    "    display(second_degree['topics_single'].value_counts())\n",
    "    display(third_degree['topics_single'].value_counts())\n",
    "\n",
    "# For the connector nodes above - just walk through each of the connections.  Do this by\n",
    "# (1) Look for first degree connections - group by subjects\n",
    "\n",
    "# get_topics_by_degree(connector_nodes)\n",
    "get_topics_by_degree(less_connector_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
